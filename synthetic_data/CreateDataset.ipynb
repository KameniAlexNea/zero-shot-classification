{
 "cells": [
  {
   "cell_type": "raw",
   "id": "1ee92826",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!uv pip install ipykernel jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4890bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517baae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import json\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8299febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = json.load(open('data/TextHard/raw_splits.json', 'r'))\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.Dataset.from_list(pairs[\"train\"])\n",
    "test = datasets.Dataset.from_list(pairs[\"test\"])\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb7a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263aebfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449cdb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(\"data/batches_cogito*/*.json\")\n",
    "loaded_files = [json.load(open(f, 'r')) for f in files]\n",
    "len(loaded_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d4b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = [sublist for sublist in loaded_files if isinstance(sublist, dict)]\n",
    "len(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb213535",
   "metadata": {},
   "outputs": [],
   "source": [
    "j[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bcb611",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([i for i in j if \"data\" in i and isinstance(i[\"data\"], list)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3eb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = [i for sublist in loaded_files if isinstance(sublist, list) for i in sublist]\n",
    "merged2 = [\n",
    "    i\n",
    "    for sublist in loaded_files\n",
    "    if isinstance(sublist, dict)\n",
    "    and \"data\" in sublist\n",
    "    and isinstance(sublist[\"data\"], list)\n",
    "    for i in sublist[\"data\"] if isinstance(i, dict)\n",
    "]\n",
    "len(merged1), len(merged2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575fa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = merged1 + merged2\n",
    "len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf6679",
   "metadata": {},
   "outputs": [],
   "source": [
    "total[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d367ea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(total)\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6909e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfdfb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~df[\"label\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3424aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df = df.dropna(subset=[\"sentence\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.columns.str.lower().str.startswith(\"label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48015f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.columns.str.lower().str.startswith(\"not\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b14bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = list(df.columns[df.columns.str.lower().str.startswith(\"label\")])\n",
    "no_labels = list(df.columns[df.columns.str.lower().str.startswith(\"not\")])\n",
    "\n",
    "def choose_non_null_label(row):\n",
    "\tfor label in labels:\n",
    "\t\tif isinstance(row[label], list) and len(row[label]) > 0:\n",
    "\t\t\treturn row[label]\n",
    "\t\telif isinstance(row[label], str) and len(row[label]) > 0:\n",
    "\t\t\treturn [row[label]]\n",
    "\treturn []\n",
    "\n",
    "def choose_non_null_not_label(row):\n",
    "\tfor label in no_labels:\n",
    "\t\tif isinstance(row[label], list) and len(row[label]) > 0:\n",
    "\t\t\treturn row[label]\n",
    "\t\telif isinstance(row[label], str) and len(row[label]) > 0:\n",
    "\t\t\treturn [row[label]]\n",
    "\treturn []\n",
    "\n",
    "df['labels'] = df.apply(choose_non_null_label, axis=1)\n",
    "df['not_labels'] = df.apply(choose_non_null_not_label, axis=1)\n",
    "df = df[['sentence', 'labels', 'not_labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e62a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fcf42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29085ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.groupby(\"sentence\")[['labels', 'not_labels']].apply(lambda x: pd.Series({\n",
    "    \"labels\": list(set(sum(x['labels'].tolist(), start=[]))),\n",
    "\t\"not_labels\": list(set(sum(x['not_labels'].tolist(), start=[])))\n",
    "})).reset_index()\n",
    "\n",
    "df_clean.shape, df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab741d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7851a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"labels\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b0733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"not_labels\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[df_clean[\"not_labels\"].apply(len) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b7d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = df_clean.apply(lambda x: set(x['labels']) & set(x['not_labels']), axis=1)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61529be",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58198ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[raw.apply(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd948b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.apply(lambda x: {\n",
    "    \"sentence\": x['sentence'],\n",
    "\t\"labels\": list(set(x['labels']) - set(x['not_labels'])),\n",
    "\t\"not_labels\": list(set(x['not_labels']) - set(x['labels']))\n",
    "}, axis=1, result_type='expand')\n",
    "\n",
    "df_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a51ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"sentence\"].apply(lambda x: x.split()).apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c632c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[\"sentence\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract all words from sentences and count their frequency\n",
    "all_words = []\n",
    "stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "for sentence in df_clean['sentence']:\n",
    "\t# Convert to lowercase and extract words (remove punctuation)\n",
    "\twords = re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower())\n",
    "\t# Filter out common stopwords\n",
    "\twords = [word for word in words if word not in stopwords]\n",
    "\tall_words.extend(words)\n",
    "\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Display most frequent words\n",
    "print(\"Top 20 most frequent words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "\tprint(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93820992",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_merged = pairs[\"train\"] + pairs[\"test\"]\n",
    "len(pairs_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa99333",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df = pd.DataFrame(pairs_merged)\n",
    "\n",
    "pairs_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0f0177",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_df[\"text\"].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_cleaned = pairs_df.groupby(\"text\")[[\"labels\"]].apply(\n",
    "    lambda x: pd.Series(\n",
    "        {\n",
    "            \"labels\": list(set(sum(x[\"labels\"].tolist(), start=[]))),\n",
    "        }\n",
    "    )\n",
    ").reset_index()\n",
    "\n",
    "pairs_cleaned.shape, pairs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e8ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_cleaned[\"labels\"].apply(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55db426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Extract all words from sentences and count their frequency\n",
    "all_words = []\n",
    "stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their'}\n",
    "\n",
    "for sentence in pairs_cleaned['text']:\n",
    "\t# Convert to lowercase and extract words (remove punctuation)\n",
    "\twords = re.findall(r'\\b[a-zA-Z]+\\b', sentence.lower())\n",
    "\t# Filter out common stopwords\n",
    "\twords = [word for word in words if word not in stopwords]\n",
    "\tall_words.extend(words)\n",
    "\n",
    "\n",
    "# Count word frequencies\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Display most frequent words\n",
    "print(\"Top 20 most frequent words:\")\n",
    "for word, count in word_counts.most_common(20):\n",
    "\tprint(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4925369",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape, pairs_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab80bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851ee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.rename(columns={\"sentence\": \"text\"})\n",
    "df_clean = df_clean[(df_clean[\"labels\"].apply(len) < 20) & (df_clean[\"not_labels\"].apply(len) < 20)]\n",
    "df_clean = df_clean.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6d6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_triplet = datasets.Dataset.from_pandas(df_clean.rename(columns={\"sentence\": \"text\"}))\n",
    "# ds_triplet = ds_triplet.rename_column(\"sentence\", \"text\")\n",
    "\n",
    "ds_couplet = datasets.Dataset.from_pandas(pairs_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de1358",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc86ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f66d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.DatasetDict({\n",
    "\t\"triplet\": ds_triplet,\n",
    "\t\"couplet\": ds_couplet\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d6d115",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Flatten all labels to count frequencies\n",
    "all_labels = sum(pairs_cleaned['labels'].tolist(), start=[])\n",
    "\n",
    "# Count label frequencies\n",
    "label_counts = Counter(all_labels)\n",
    "total_labels = len(all_labels)\n",
    "\n",
    "# Define rare labels (bottom 0.2% by frequency)\n",
    "sorted_labels = sorted(label_counts.items(), key=lambda x: x[1])\n",
    "rare_threshold_idx = int(len(sorted_labels) * 0.2)  # 0.2%\n",
    "rare_labels = set([label for label, count in sorted_labels[:rare_threshold_idx]])\n",
    "\n",
    "print(f\"Total unique labels: {len(label_counts)}\")\n",
    "print(f\"Rare labels (bottom 0.2%): {len(rare_labels)}\")\n",
    "print(f\"Rare labels: {list(rare_labels)[:10]}...\")  # Show first 10\n",
    "\n",
    "# Vectorized operation to identify rows with rare labels\n",
    "has_rare_labels = pairs_cleaned['labels'].apply(lambda labels_list: any(label in rare_labels for label in labels_list))\n",
    "test_indices = has_rare_labels[has_rare_labels].index.tolist()\n",
    "\n",
    "# Create test set with rare labels only - vectorized filtering\n",
    "test_mask = pairs_cleaned.index.isin(test_indices)\n",
    "test_subset = pairs_cleaned[test_mask].copy()\n",
    "\n",
    "# Filter labels to keep only rare ones using vectorized operation\n",
    "test_subset['labels'] = test_subset['labels'].apply(lambda labels_list: [label for label in labels_list if label in rare_labels])\n",
    "\n",
    "# Remove rows where no rare labels remain after filtering\n",
    "test_df = test_subset[test_subset['labels'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "# Create train set (all rows not in test)\n",
    "train_df = pairs_cleaned[~test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Test set labels sample: {test_df['labels'].iloc[0] if len(test_df) > 0 else 'No test data'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e15042",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_couplet = datasets.DatasetDict({\n",
    "\t\"train\": datasets.Dataset.from_pandas(train_df),\n",
    "\t\"test\": datasets.Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "ds_couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Flatten all labels to count frequencies\n",
    "all_labels = sum(df_clean['labels'].tolist(), start=[])\n",
    "\n",
    "# Count label frequencies\n",
    "label_counts = Counter(all_labels)\n",
    "total_labels = len(all_labels)\n",
    "\n",
    "# Define rare labels (bottom 0.2% by frequency)\n",
    "sorted_labels = sorted(label_counts.items(), key=lambda x: x[1])\n",
    "rare_threshold_idx = int(len(sorted_labels) * 0.2)  # 0.2%\n",
    "rare_labels = set([label for label, count in sorted_labels[:rare_threshold_idx]])\n",
    "\n",
    "print(f\"Total unique labels: {len(label_counts)}\")\n",
    "print(f\"Rare labels (bottom 0.2%): {len(rare_labels)}\")\n",
    "print(f\"Rare labels: {list(rare_labels)[:10]}...\")  # Show first 10\n",
    "\n",
    "# Vectorized operation to identify rows with rare labels\n",
    "has_rare_labels = df_clean['labels'].apply(lambda labels_list: any(label in rare_labels for label in labels_list))\n",
    "test_indices = has_rare_labels[has_rare_labels].index.tolist()\n",
    "\n",
    "# Create test set with rare labels only - vectorized filtering\n",
    "test_mask = df_clean.index.isin(test_indices)\n",
    "test_subset = df_clean[test_mask].copy()\n",
    "\n",
    "# Filter labels to keep only rare ones using vectorized operation\n",
    "test_subset['labels'] = test_subset['labels'].apply(lambda labels_list: [label for label in labels_list if label in rare_labels])\n",
    "\n",
    "# Remove rows where no rare labels remain after filtering\n",
    "test_df = test_subset[test_subset['labels'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "# Create train set (all rows not in test)\n",
    "train_df = df_clean[~test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nTrain set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")\n",
    "print(f\"Test set labels sample: {test_df['labels'].iloc[0] if len(test_df) > 0 else 'No test data'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1194f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_triplet = datasets.DatasetDict({\n",
    "\t\"train\": datasets.Dataset.from_pandas(train_df),\n",
    "\t\"test\": datasets.Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "ds_triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729a2ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.DatasetDict({\n",
    "\t\"triplet\": ds_triplet,\n",
    "\t\"couplet\": ds_couplet\n",
    "})\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519e380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds.push_to_hub(\"alexneakameni/ZSHOT-HARDSET\", commit_message=\"alexneakameni: Initial commit of ZSHOT-HARDSET dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a853dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push each subset under a different config name\n",
    "ds[\"triplet\"].push_to_hub(\n",
    "    \"alexneakameni/ZSHOT-HARDSET\",\n",
    "    # split=\"triplet\",\n",
    "    config_name=\"triplet\",\n",
    "    commit_message=\"alexneakameni: Upload triplet subset -- updated\",\n",
    ")\n",
    "ds[\"couplet\"].push_to_hub(\n",
    "    \"alexneakameni/ZSHOT-HARDSET\",\n",
    "    # split=\"couplet\",\n",
    "    config_name=\"couplet\",\n",
    "    commit_message=\"alexneakameni: Upload couplet subset -- updated\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f709858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
